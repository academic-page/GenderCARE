<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <meta name="title" content="GenderCARE: A Comprehensive Framework for Assessing and Reducing Gender Bias in LLMs">
  <meta name="description" content="A comprehensive framework with innovative criteria, bias assessment, reduction techniques, and evaluation metrics for quantifying and mitigating gender bias in Large Language Models">
  <meta name="keywords" content="gender bias, large language models, LLMs, fairness, GenderPair, bias mitigation, AI ethics, NLP">
  <meta name="author" content="Kunsheng Tang, Wenbo Zhou, Jie Zhang">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <meta property="og:site_name" content="University of Science and Technology of China">
  <meta property="og:title" content="GenderCARE: A Comprehensive Framework for Assessing and Reducing Gender Bias in LLMs">
  <meta property="og:description" content="A comprehensive framework for quantifying and mitigating gender bias in Large Language Models">
  <meta property="og:url" content="https://kstanghere.github.io/gendercare">
  <meta property="og:image" content="https://kstanghere.github.io/gendercare/static/images/social_preview.png">
  
  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="GenderCARE: A Comprehensive Framework for Assessing and Reducing Gender Bias in LLMs">
  <meta name="twitter:description" content="A comprehensive framework for quantifying and mitigating gender bias in Large Language Models">
  
  <title>GenderCARE - Gender Bias Assessment and Reduction in LLMs</title>
  
  <!-- Favicon -->
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  
  <!-- CSS -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  
  <!-- Fonts -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <!-- Chart.js for radar charts -->
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.0/dist/chart.umd.min.js"></script>
  
  <!-- JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/index.js"></script>
  <script defer src="static/js/results.js"></script>
  
  <style>
    .author-block a {
      color: #3273dc;
      text-decoration: none;
    }
    .author-block a:hover {
      text-decoration: underline;
    }
    .framework-section {
      padding: 3rem 1.5rem;
    }
    .framework-image {
      max-width: 100%;
      height: auto;
      margin: 2rem auto;
      display: block;
    }
    .results-container {
      margin-top: 2rem;
    }
    .model-selector {
      margin-bottom: 2rem;
      text-align: center;
    }
    .model-selector select {
      padding: 0.75rem 1.5rem;
      font-size: 1.1rem;
      border-radius: 4px;
      border: 2px solid #dbdbdb;
      background-color: white;
      cursor: pointer;
      min-width: 300px;
    }
    .model-selector select:hover {
      border-color: #3273dc;
    }
    .charts-grid {
      display: grid;
      grid-template-columns: 1fr 1fr;
      gap: 2rem;
      margin-top: 2rem;
    }
    .chart-container {
      background: white;
      padding: 1.5rem;
      border-radius: 8px;
      box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    }
    .chart-title {
      text-align: center;
      font-size: 1.3rem;
      font-weight: 600;
      margin-bottom: 1rem;
      color: #363636;
    }
    @media (max-width: 768px) {
      .charts-grid {
        grid-template-columns: 1fr;
      }
    }
    .sample-image {
      max-width: 100%;
      height: auto;
      margin: 2rem auto;
      display: block;
      border: 1px solid #dbdbdb;
      border-radius: 4px;
    }
  </style>
</head>
<body>

  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <main id="main-content">
  
  <!-- Title and Authors Section -->
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">GenderCARE: A Comprehensive Framework for Assessing and Reducing Gender Bias in Large Language Models</h1>
            
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="mailto:kstang@mail.ustc.edu.cn" target="_blank">Kunsheng Tang</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="mailto:welbeckz@ustc.edu.cn" target="_blank">Wenbo Zhou</a><sup>1,*</sup>,
              </span>
              <span class="author-block">
                <a href="mailto:zhang_jie@cfar.a-star.edu.sg" target="_blank">Jie Zhang</a><sup>2,*</sup>,
              </span>
              <span class="author-block">
                <a href="mailto:liuaishan@buaa.edu.cn" target="_blank">Aishan Liu</a><sup>3</sup>,
              </span>
              <span class="author-block">
                <a href="mailto:gdeng003@e.ntu.edu.sg" target="_blank">Gelei Deng</a><sup>4</sup>,
              </span>
              <span class="author-block">
                <a href="mailto:li_shuai@mail.ustc.edu.cn" target="_blank">Shuai Li</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="mailto:qipeigui@mail.ustc.edu.cn" target="_blank">Peigui Qi</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="mailto:zhangwm@ustc.edu.cn" target="_blank">Weiming Zhang</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="mailto:tianwei.zhang@ntu.edu.sg" target="_blank">Tianwei Zhang</a><sup>4</sup>,
              </span>
              <span class="author-block">
                <a href="mailto:ynh@ustc.edu.cn" target="_blank">Nenghai Yu</a><sup>1</sup>
              </span>
            </div>

            <div class="is-size-6 publication-authors" style="margin-top: 1rem;">
              <span class="author-block"><sup>1</sup>University of Science and Technology of China</span><br>
              <span class="author-block"><sup>2</sup>CFAR and IHPC, A*STAR Singapore</span><br>
              <span class="author-block"><sup>3</sup>Beihang University</span><br>
              <span class="author-block"><sup>4</sup>Nanyang Technological University</span><br>
              <span class="eql-cntrb"><small><sup>*</sup>Indicates Equal Contribution</small></span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links" style="margin-top: 1.5rem;">
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2408.12494.pdf" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                  </a>
                </span>

                <span class="link-block">
                  <a href="https://github.com/kstanghere/GenderCARE-ccs24" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
                </span>

                <span class="link-block">
                  <a href="https://arxiv.org/abs/2408.12494" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Framework Overview -->
  <section class="section framework-section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column">
          <img src="static/images/gendercare.png" alt="GenderCARE Framework" class="framework-image">
          <p class="has-text-justified" style="margin-top: 1rem;">
            The GenderCARE framework for comprehensive gender bias assessment and reduction in LLMs. It consists of four key components: (I) <strong>Criteria</strong> for gender equality benchmarks; (II) <strong>Assessment</strong> of gender bias in LLMs using the proposed GenderPair benchmark aligned with the criteria; (III) <strong>Reduction</strong> of gender bias via counterfactual data augmentation and fine-tuning strategies; (IV) <strong>Evaluation</strong> metrics at both lexical and semantic levels for bias quantification.
          </p>
        </div>
      </div>
    </div>
  </section>

  <!-- Abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Large language models (LLMs) have exhibited remarkable capabilities in natural language generation, but they have also been observed to magnify societal biases, particularly those related to gender. In response to this issue, several benchmarks have been proposed to assess gender bias in LLMs. However, these benchmarks often lack practical flexibility or inadvertently introduce biases.
            </p>
            <p>
              To address these shortcomings, we introduce <strong>GenderCARE</strong>, a comprehensive framework that encompasses innovative <strong>C</strong>riteria, bias <strong>A</strong>ssessment, <strong>R</strong>eduction techniques, and <strong>E</strong>valuation metrics for quantifying and mitigating gender bias in LLMs. To begin, we establish pioneering criteria for gender equality benchmarks, spanning dimensions such as inclusivity, diversity, explainability, objectivity, robustness, and realisticity. Guided by these criteria, we construct GenderPair, a novel pair-based benchmark designed to assess gender bias in LLMs comprehensively.
            </p>
            <p>
              Our benchmark provides standardized and realistic evaluations, including previously overlooked gender groups such as transgender and non-binary individuals. Furthermore, we develop effective debiasing techniques that incorporate counterfactual data augmentation and specialized fine-tuning strategies to reduce gender bias in LLMs without compromising their overall performance. Extensive experiments demonstrate a significant reduction in various gender bias benchmarks, with reductions peaking at over 90% and averaging above 35% across 17 different LLMs. Importantly, these reductions come with minimal variability in mainstream language tasks, remaining below 2%.
            </p>
            <p>
              By offering a realistic assessment and tailored reduction of gender biases, we hope that our <strong>GenderCARE</strong> can represent a significant step towards achieving fairness and equity in LLMs.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- GenderPair Benchmark Statistics -->
  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">GenderPair Benchmark Statistics</h2>
      <div class="content">
        <p class="has-text-justified" style="margin-bottom: 2rem;">
          Summary of the elements in the pair set utilized by the <strong>GenderPair</strong> benchmark. We delineate the distribution of gender targets, biased and anti-biased descriptors, and prompts across three distinct gender groups. The details of each element are documented in the appendix, available at our GitHub repository.
        </p>
        
        <div class="table-container">
          <table class="table is-bordered is-striped is-fullwidth">
            <thead>
              <tr>
                <th rowspan="2" style="vertical-align: middle;">Gender Groups</th>
                <th colspan="4" style="text-align: center;">Gender Targets</th>
                <th rowspan="2" style="vertical-align: middle;"># Biased Descriptors</th>
                <th rowspan="2" style="vertical-align: middle;"># Anti-Biased Descriptors</th>
                <th rowspan="2" style="vertical-align: middle;"># Prompts</th>
              </tr>
              <tr>
                <th># Identities</th>
                <th># Titles</th>
                <th># Pronouns</th>
                <th># Names</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td><strong>Group 1</strong></td>
                <td>5</td>
                <td>25</td>
                <td>4</td>
                <td>30</td>
                <td>83</td>
                <td>83</td>
                <td>31,872</td>
              </tr>
              <tr>
                <td><strong>Group 2</strong></td>
                <td>5</td>
                <td>25</td>
                <td>4</td>
                <td>30</td>
                <td>83</td>
                <td>83</td>
                <td>31,872</td>
              </tr>
              <tr>
                <td><strong>Group 3</strong></td>
                <td>10</td>
                <td>23</td>
                <td>18</td>
                <td>30</td>
                <td>83</td>
                <td>83</td>
                <td>40,338</td>
              </tr>
            </tbody>
          </table>
        </div>
      </div>
    </div>
  </section>

  <!-- GenderPair Examples -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">GenderPair Prompt Examples</h2>
      <div class="content">
        <p class="has-text-justified" style="margin-bottom: 1rem;">
          Some prompt instances of the proposed <strong>GenderPair</strong> benchmark. Each instance is constructed as [<span style="color: #3273dc;">instruction</span> & <span style="color: #c53030;">pair set</span> & <span style="color: #805ad5;">requirement</span>]. For different configurations of pair set, we provide two options for instruction.
        </p>
        <img src="static/images/table_genderpair_examples.png" alt="GenderPair Examples" class="sample-image">
      </div>
    </div>
  </section>

  <!-- Results Section -->
  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">Debiasing Results</h2>
      <div class="content has-text-justified" style="margin-bottom: 2rem;">
        <p>
          Interactive visualization of gender bias metrics before and after applying GenderCARE debiasing techniques. Select a model to compare the <strong>Bias-Pair Ratio</strong>, <strong>Toxicity</strong>, and <strong>Regard-Negative</strong> scores. Lower values indicate better performance (i.e., less bias).
        </p>
      </div>
      
      <div class="results-container">
        <div class="model-selector">
          <label for="modelSelect" style="font-size: 1.1rem; margin-right: 1rem;"><strong>Select Model:</strong></label>
          <select id="modelSelect">
            <option value="">-- Choose a model --</option>
          </select>
        </div>
        
        <div class="charts-grid" id="chartsContainer" style="display: none;">
          <div class="chart-container">
            <div class="chart-title">Before Debiasing</div>
            <canvas id="beforeChart"></canvas>
          </div>
          <div class="chart-container">
            <div class="chart-title">After Debiasing</div>
            <canvas id="afterChart"></canvas>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- BibTeX Citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <div class="bibtex-header">
        <h2 class="title">BibTeX</h2>
        <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
          <i class="fas fa-copy"></i>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <pre id="bibtex-code"><code>@article{tang2024gendercare,
  title={GenderCARE: A Comprehensive Framework for Assessing and Reducing Gender Bias in Large Language Models},
  author={Tang, Kunsheng and Zhou, Wenbo and Zhang, Jie and Liu, Aishan and Deng, Gelei and Li, Shuai and Qi, Peigui and Zhang, Weiming and Zhang, Tianwei and Yu, Nenghai},
  journal={arXiv preprint arXiv:2408.12494},
  year={2024}
}</code></pre>
    </div>
  </section>

  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content has-text-centered">
            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>.
              This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

  </main>
</body>
</html>